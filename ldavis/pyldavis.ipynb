{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba as jb, matplotlib as mpl, matplotlib.pyplot as plt, numpy as np, os, pandas as pd, re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [',', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', \n",
    "             '（', '）', ',', ':', 'hi', 'auntie', 'ok', '向左走', '向右走', '大家', '利申', \n",
    "             '雖然', '但係', '乜', '一齊', '可以', '應該', '好多', '已經', '因為', '邊個',\n",
    "             '好似', '而家', '一定', '之前', '即刻', '好過', '仲有', '如果', '其實', '一半',\n",
    "             '有人', '個人', '一次', '無人', '好好', '根本', '一樣', '成日', '問題', '不過',\n",
    "             '有時', '之後', '沒有', '所以', '不如', '個個', '無法']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_token(sent, StopWords=True, RemoveHttp=True):\n",
    "  if RemoveHttp == True:\n",
    "    sent = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sent, flags=re.MULTILINE)\n",
    "  words = '/'.join(jb.cut(sent)).split('/')\n",
    "  if StopWords == True:\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfvectorizer(words_list, max_features=1000, n_top_words=50, n_components=10, return_model=False):\n",
    "  def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "      print(\"Topic #%d:\" % topic_idx)\n",
    "      print(\" \".join([feature_names[i]\n",
    "                      for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "  sents = []\n",
    "  for words in words_list:\n",
    "    sents.append(' '.join(words))\n",
    "  vtr = CountVectorizer(max_df=0.85, min_df=2,\n",
    "                        max_features=max_features)\n",
    "  vtr_sents = vtr.fit_transform(sents)\n",
    "\n",
    "  lda = LatentDirichletAllocation(n_components=n_components, max_iter=10,\n",
    "                                  learning_method='online',\n",
    "                                  learning_offset=50,\n",
    "                                  random_state=12345)\n",
    "  lda.fit(vtr_sents)\n",
    "  if return_model == True:\n",
    "    return lda, vtr_sents, vtr\n",
    "  vtr_feature_names = vtr.get_feature_names()\n",
    "  #print_top_words(lda, vtr_feature_names, n_top_words)\n",
    "  lda_words_list = []\n",
    "  for topic_idx, topic in enumerate(lda.components_):\n",
    "    term = [topic_idx, [vtr_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]]\n",
    "    lda_words_list.append(term)\n",
    "  return lda_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing():\n",
    "  words_list = []\n",
    "  data_files = []\n",
    "  folder = './input/'\n",
    "  for subdir, dirs, files in os.walk(folder):\n",
    "    for f in files:\n",
    "      data_files.append(subdir + f)\n",
    "  for f in data_files:\n",
    "    df = pd.read_csv(f)\n",
    "    sents = df.response.values\n",
    "    for sent in sents:\n",
    "      if not isinstance(sent, str):\n",
    "        continue\n",
    "      sent = sent.split('|')\n",
    "      for s in sent:\n",
    "        words_list.append(sent_token(s))\n",
    "        \n",
    "  return tfidfvectorizer(words_list, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_lda, sklearn_wordvec, sklearn_wordvecmodel = data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "pyLDA = pyLDAvis.sklearn.prepare(sklearn_lda, sklearn_wordvec, sklearn_wordvecmodel, mds='tsne')\n",
    "pyLDAvis.save_html(pyLDA, 'ldavis_golden.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
